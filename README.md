# Autoencoder-Demonstrations

This repository will be used as a central hub for containing various different demonstration applications for the use of autoencoders in varying use-cases. The subdirectories of this project are the varying use-cases which have currently been completed. Each subdirectory has its own readme, which more thoroughly describes the cotnents of the project or sub-projects within.

# Use-Case: Feature Extraction

To extract or not to extract... that is the question. The decision of whether to reduce the feature space of inputs into machine learning models has been a well-researched topic. I personally performed a high volume of research into this field myself during my Master's Program. My research at the time concluded that there are tradeoffs -- using the unaltered images results in slower training, larger model files, greater runtime overhead for inference, and more system memory/computation requirements than the use of feature-extracted represetnations. The autoencoder method trains an intermediary model in order to learn how to encode input values to a much smaller feature space (we encoded the $28 \times 28$ MNIST handwirtten digits dataset down to $7 \times 7$ to reduce the number of features to one sixteenth the original amount). The autoencoder also learns the appropriate way to reconstruct the original images from its own encodings generated for the images. This makes the autoencoder essentially an encoder and a decoder stacked together. This subdirectory contains various subprojects which demonstrate the use of such autoencoders in action. See the directory's readme for more information on the projects within.
